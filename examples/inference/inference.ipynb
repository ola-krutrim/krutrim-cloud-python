{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc26ec39-9c94-4751-bff6-ef9092a70704",
   "metadata": {},
   "source": [
    "# Inference SDK on Krutrim Cloud\n",
    "This notebook demonstrates how to utilize the Inference SDK on Krutrim Cloud for managing and executing inference tasks. It provides step-by-step instructions for operations such as listing available clusters, managing fine-tuning checkpoints, creating inference tasks, retrieving task information, and monitoring the status of ongoing tasks. By following this guide, users can effectively set up, execute, and manage inference processes within the Krutrim Cloud environment, ensuring optimal performance and resource utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83dd7b8-3def-4a1e-825e-acfbb69dd11a",
   "metadata": {},
   "source": [
    "## Install Krutrim Cloud SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c76a95-1948-4a28-864f-a93187776b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install krutrim-cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72acf22c-8599-41e8-937c-51aae32743b1",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "**Export the Required Environment Variables:** Create a .env file in the examples directory with the following details:\n",
    "\n",
    "- KRUTRIM_CLOUD_API_KEY=\"Your Krutrim Cloud API Key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f5b421-4fcc-44e6-bbda-850e661cac91",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Environment Variables\n",
    "- **Purpose**: To prepare the environment for the script.\n",
    "- **Key Actions**:\n",
    "    -  Import necessary libraries (krutrim_cloud for API access, dotenv for environment variables).\n",
    "    - Load environment variables from a .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c68d96-08c3-4e24-802b-b2fef985dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from krutrim_cloud import KrutrimCloud\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d632f8-4fa3-41a0-9696-3f0f63429834",
   "metadata": {},
   "source": [
    "## Initialize KrutrimCloud Client\n",
    "- **Purpose:** To set up the API client for making requests.\n",
    "- **Key Actions:**\n",
    "    - Create an instance of KrutrimCloud using the base URL from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81203050-d3e7-4c24-b539-053d592fe645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KrutrimCloud client\n",
    "client = KrutrimCloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726725a-b30e-4b05-b5b9-1343017b4c14",
   "metadata": {},
   "source": [
    "## List Fine-Tuning Checkpoints\n",
    "- **Purpose**: Retrieve and display all fine-tuning checkpoints to monitor available models for inference.\n",
    "- **Key Actions**:\n",
    "    -  Fetches fine-tuning checkpoints and manages potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f473383-0bc8-4cb8-b9e7-0e156c55ccf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CheckpointListResponseItem(model='Meta-Llama-3-8B-instruct', name='ft-9ea84101-4cee-4b80-8b31-5e67d8628b98-ft_task_1_1_final', version='ft')]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # List all fine-tuning checkpoints\n",
    "    fine_tuning_checkpoints = client.inference.checkpoints.list()\n",
    "    print(fine_tuning_checkpoints)  # Print or process the checkpoints list\n",
    "except Exception as e:\n",
    "    # Handle any exceptions that occur during the API call\n",
    "    print(f\"An error occurred while fetching the fine-tuning checkpoints: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d3af79-badf-4c74-8c96-846a5d28b305",
   "metadata": {},
   "source": [
    "## Get Inference Task Information\n",
    "- **Purpose**: Retrieve and display information for a specific inference task by ID.\n",
    "- **Key Actions**:\n",
    "    -  Retrieves task information and includes validation error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db6dcabe-390e-4986-a590-199cb817ac17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheckpointRetrieveResponse(ctime='25_09_2024_15_41_25_967212', dataset='ft-alpaca-tiny.json', epoch='1', mode='lora', model='Meta-Llama-3-8B-instruct', mtime='25_09_2024_15_48_33_549117', name='ft_task_1', status='succeed', steps='steps', test_dataset='test-dataset')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    filename = fine_tuning_checkpoints[0].name\n",
    "    # Get Inference Task Information\n",
    "    inference_task_info = client.inference.checkpoints.retrieve(filename=filename)\n",
    "    print(inference_task_info)  # Print or process the retrieved inference task information\n",
    "except Exception as e:\n",
    "    # Handle any exceptions that occur during the API call\n",
    "    print(f\"An error occurred while fetching the inference task information: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c0437-fa20-4d72-bdf0-4f6c511b50f9",
   "metadata": {},
   "source": [
    "## Delete a Checkpoint\n",
    "- **Purpose**: Delete a specified fine-tuning checkpoint.\n",
    "- **Key Actions**:\n",
    "    -  Deletes a checkpoint and handles errors during the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd17d9-929b-41da-818f-7f336a7cddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    filename = fine_tuning_checkpoints[0].name\n",
    "    # Delete the checkpoint\n",
    "    delete_response = client.inference.checkpoints.delete(filename=filename)\n",
    "    print(delete_response)  # Print the response after deletion, if needed\n",
    "except Exception as e:\n",
    "    # Handle any other exceptions that are not specifically caught above\n",
    "    print(f\"An unexpected error occurred while deleting the checkpoint: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c8400-b519-41d8-8bf3-f8849caa4587",
   "metadata": {},
   "source": [
    "## Create Inference Task\n",
    "- **Purpose**: Create a new inference task with specified parameters.\n",
    "- **Key Actions**:\n",
    "    -  Initiates a new inference task and handles any arising errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46a99a90-8437-41c7-8854-fedd7a917d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskCreateResponse(id='0239299e-00bd-412e-8588-d89245299992', name='ft_task_1')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    checkpoint = fine_tuning_checkpoints[0].name\n",
    "    model = inference_task_info.name\n",
    "    # Create Inference Task\n",
    "    inference_task_response = client.inference.tasks.create(\n",
    "        model=model,\n",
    "        namespace=\"gpu-scheduler\",\n",
    "        priority=1,\n",
    "        checkpoint=checkpoint,\n",
    "        ngpu=1,\n",
    "        min_replicas=1,\n",
    "        max_replicas=1,\n",
    "        max_batch_size=256\n",
    "    )\n",
    "    print(inference_task_response)\n",
    "except Exception as e:\n",
    "    # Handle any exceptions that occur during the API call\n",
    "    print(f\"An error occurred while creating the inference task: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9897d20b-73b0-4874-9de2-e625b2bed727",
   "metadata": {},
   "source": [
    "## List Inference Tasks\n",
    "- **Purpose**: Retrieve and display all current inference tasks.\n",
    "- **Key Actions**:\n",
    "    -  Fetches the list of inference tasks and manages errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa4322d-f9cc-427a-9750-3c7dfccab032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskListResponse(count=1, offset=0, task_list=[{'name': 'ft_task_1', 'basemodel': 'Meta-Llama-3-8B-instruct', 'id': '0239299e-00bd-412e-8588-d89245299992', 'status': 'init', 'mtime': '09/27/2024 04_49_21 UTC'}])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # List Inference Tasks\n",
    "    inference_task_list = client.inference.tasks.list()\n",
    "    print(inference_task_list)\n",
    "except Exception as e:\n",
    "    # Handle any exceptions that occur during the API call\n",
    "    print(f\"An error occurred while listing the inference tasks: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc145f39-ed5f-4e09-926a-f5793991a12e",
   "metadata": {},
   "source": [
    "## List Inference Tasks by ID\n",
    "- **Purpose**: Retrieve and display information for a specific inference task by ID to check its status and details.\n",
    "- **Key Actions**:\n",
    "    -  Retrieves task information and includes validation error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b65e4283-abe8-4438-ab3c-27f27f718930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskRetrieveResponse(id='0239299e-00bd-412e-8588-d89245299992', basemodel='Meta-Llama-3-8B-instruct', checkpoint='ft-9ea84101-4cee-4b80-8b31-5e67d8628b98-ft_task_1_1_final', inference_svc_name=None, name='ft_task_1', namespace='gpu-scheduler', priority=1, status='init')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    id = inference_task_list.task_list[0].get(\"id\")\n",
    "    # Get Inference Task Information\n",
    "    task_info = client.inference.tasks.retrieve(id=id)\n",
    "    print(task_info)\n",
    "except Exception as e:\n",
    "    # Handle any exceptions that occur during the API call\n",
    "    print(f\"An error occurred while retrieving the inference task information: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a15e3e-7862-4cb1-9afd-0e762f4600f6",
   "metadata": {},
   "source": [
    "## Cancel Inference Task\n",
    "- **Purpose**: Cancel a specified inference task by ID to stop unnecessary processing or free up resources.\n",
    "- **Key Actions**:\n",
    "    -  Cancels an inference task and handles exceptions during the cancellation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26cfe494-6d3d-4a03-abc2-b14b68c46664",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    id = inference_task_list.task_list[0].get(\"id\")\n",
    "    # Cancel Inference Task\n",
    "    cancel_response = client.inference.tasks.cancel(id=id)\n",
    "except Exception as e:\n",
    "    # Handle any exceptions that occur during the API call\n",
    "    print(f\"An error occurred while canceling the inference task: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
